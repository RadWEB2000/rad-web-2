import {Hero} from '../../../components/Utilities/Fragments/Post/Hero/Hero';
import {Content} from '../../../components/Utilities/Fragments/Post/Content/Content' ;
import {Movie} from '../../../components/Utilities/Fragments/Post/Movie/Movie';
import {Poster} from '../../../components/Utilities/Fragments/Post/Poster/Poster';
import { AuthorCard } from "../../../components/Utilities/Cards/AuthorCard/AuthorCard"
import { Seo } from "../../../components/Layout/Seo/Seo";

<Seo
    canonical='https://rad-web.vercel.app/blog/fenomen-windowsa'
    description='Roboty wyszukiwarek internetowych pracują dzień i noc, jednak czy można je kontrolować? Zapraszam'
    image='https://images.unsplash.com/photo-1555255707-c07966088b7b?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1632&q=80'
    title='Robots.txt i kontrola nad robotami wyszukiwarek - RadWEB'
    typePage='article'
/>
<Hero
    title='Robots.txt — Jak przejąć kontrolę nad robotami wyszukiwarek?'
    image='https://images.unsplash.com/photo-1555255707-c07966088b7b?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1632&q=80'
    release='Sun Oct 02 2022 17:40:26 GMT+0200 (czas środkowoeuropejski letni)'
>
Od końca lat 60. XX wieku, czyli od momentu stworzenia internetu powstało prawie 2 miliardy stron internetowych. Sporo. W związku z tym, kiedy internet stał się globalnym dobrem, stworzono programy — roboty wyszukiwarek, które miały i mają nadal za zadanie skanować wszystkie strony internetowe w ramach wyszukiwarki internetowej oraz filtrować je. Po przefiltrowaniu stron boty wyszukiwarek sortują strony internetowe w swoim rankingu. Czym są roboty wyszukiwarek? Jak zarządzać nimi? Szczegóły poznasz poniżej.
</Hero>
<Content title='Czym są roboty?'>
Każdy z nas słyszał o robotach. To dzieło techniki rozpropagowały filmy z gatunku  „Ja Robot”. Na szerszą, światową skalę roboty pod sformułowaniem droid — słowo, do którego Disney/LucasArts posiadają prawa autorskie — rozpropagowały filmy z sagi  „Gwiezdne Wojny”.
<br/>
### Robot — naukowa definicja
Robot to nic innego jak maszyna, bądź też system komputerowy wyposażony w program sterujący peryferiami. To zadanie ma na celu wykonanie wcześniej predefiniowanego zadania, jak chociażby nastawienie prania. 
<br/>
### Maszyny ze sztuczną inteligencją
Wraz z postępem technologicznym roboty są wyposażane, w coraz bardziej zaawansowaną sztuczną inteligencję. **AI, czyli sztuczna inteligencja umożliwia robotom rozwiązywanie często abstrakcyjnych algorytmów, trudnych do natychmiastowego wyliczenia dla człowieka.** Dziś każdy, który posiada chęć oraz zapał może stworzyć własnego robota, który będzie rozwiązywał problemy życia codziennego. 
</Content>
<Content title='Co to są boty i czym się zajmują?'>
  **Boty — skrócone, w tym kontekście zamienne słowo dla robota** — istnieją co najmniej od czasu, kiedy wyprodukowaną pierwszą grę, w której musieliśmy zmierzyć się z wirtualnym przeciwnikiem. Dziś tacy wirtualni przeciwnicy wykonujący tysiące obliczeń przed wykonaniem akcji są bliscy wykonywania takich samych akcji, co gracze zasiadający za klawiaturą bohatera. 
<br/>
  W świecie technologii informatycznych **wyraz bot definiuje oprogramowanie — jak wyżej wykazałem — który ma na celu odwzorować czynności żywego użytkownika**. Jednymi z obecnie najpopularniejszych botów, są chatboty oraz voiceboty, Chatboty można znaleźć dla przykładu w komunikatorach internetowych, takich jak Discord lub Messenger.
</Content>
<Content title='Robot internetowy, czyli robot indeksujący'>
Robot internetowy nazywany również robotem indeksującym jest oprogramowaniem, które nieustannie zbiera informacje o:
- strukturze stron internetowych
- treściach stron internetowych
- grafikach i innych rodzajach multimediów

Należy pamiętać, że przeznaczenie robota determinuje jego zachowanie. Bot odpowiadający za tworzenie bazy kontaktów bazujących na adresach e-mail - niezwykle popularny mechanizm przy newsletterach.
</Content>
<Content title="Crawlowanie — Mechanizm działania robotów wyszukiwarek na stronach internetowych">
Zrozumienie działania mechanizmu zachowań robotów indeksujących na stronach internetowych jest kluczowy dla pozycjonerów i działań SEO. Crawl to proces przeszukiwania internetu mający na celu znaleźć strony internetowe oraz unikalne treści znajdujące się w internecie. Przykładem takich treści mogą być informacje z kraju oraz ze świata, a także grafiki i inne multimedia. Po odnalezieniu plików źródłowych roboty indeksujące skanują pliki. 
<br/>
### Jakie istnieją rodzaje zachowań robotów wyszukiwarek:
- ### Fresh crawl
   <div style={{textIndent:'2rem'}}> Mechanizm robotów wyszukiwarek, które nawet do kilku razy dziennie weryfikuje zmiany na stronie internetowej.</div>
- ### Deep crawl
     <div style={{textIndent:'2rem'}}>Wymusza na robotach indeksujących pobieranie i analizowanie całych witryn internetowych do kilku razy miesięcznie.</div>
<br/>
Po pozytywnym zakończeniu procesu analizowania strony internetowej przez boty wyszukiwarek możemy oczekiwać, że algorytm wyszukiwarek zaindeksuje adres www.
</Content>
<Content title="Canonical — linki kanoniczne">
Jedynym z najważniejszych tagów (tak są określane znaczniki w HTML) jest meta tag  przedstawienia właściciela treści na stronie internetowej. Dlaczego jest to tak istotne? Wyszukiwarki internetowe karzą strony internetowe, które skopiowały zawartość z innego serwisu lub przedstawiają bardzo podobną zawartość.
<br/>
Przykładem, gdzie najczęściej można się spotkać z problemem duplikowania treści, jest paginacja, czyli numerowanie stron z artykułami blogowymi lub produktami w sklepie internetowym, gdzie zmieniają się jedynie posty lub asortyment, ale pozostała zawartość jest bez zmian. Aby uniknąć kary od robotów indeksujących, jest poniższy przykład kodu:

```
<link rel="canonical" href=”https://www.example.com/adres.php” /> 
<link rel="canonical" href=”/adres.php” /> 
```
</Content>

<Content title="Meta robots — wskazywanie robotom sposobu postępowania">
Meta robots to kolejny meta tag w tym artykule, z tym że on już konkretnie podaje poniższe polecenia nazywane dyrektywami robotom wyszukiwarek:
- **index, follow** — wszystko zostanie zaindeksowane,
- **index** — strona zostanie zaindeksowana,
- **follow** — zostaną zaindeksowane linki w celu późniejszego -  odwiedzenia przez roboty,
- **index, nofollow** — strona zostanie zaindeksowana z wyjątkiem znajdujących się na niej linków,
- **noindex, follow** — zostaną zaindeksowane jedynie linki,
- **noindex** — strona nie zostanie zaindeksowana,
- **nofollow** — linki znajdujące się na stronie nie zostaną zaindeksowane,
- **noindex, nofollow** — roboty nic nie będą indeksować

Warto wiedzieć, że domyślną wartością meta tagu robots jest index,follow.

```
<meta name=”robots” content=”index,follow” /> 
```

</Content>
<Content title="User-Agent, czyli ukryty agent wyszukiwarek internetowych ">
Przed przejściem do konfigurowania już wielokrotnie wspominanego robots.txt należy jeszcze uzupełnić swoją wiedzę o User-Agent, gdzie dalej będę naprzemiennie używał tego sformułowania ze skrótem UA. User agent jest nagłówkiem identyfikującym, wysyłanym przez protokół HTTP lub aplikację taką jak przeglądarka internetowa albo bot. Służy on do rozpoznawania typu programu klienckiego oraz do budowania statystyk odwiedzin stron internetowych.

Przykłady najpopularniejszych nagłówków UA:
- Googlebot/2.1
- Mozilla/5.0
- Mediapartners-Google/2.1
- JavaX.X.X
</Content>
<Content title="Dlaczego powstał i czym jest plik robots.txt ? ">
Robots.txt jest plikiem tekstowym, który zazwyczaj powinien się znajdować w głównym katalogu domeny  (/robots.txt). W tym pliku przechowywane są instrukcje i dyrektywy dla robotów wyszukiwarek.

Robots.txt powstał w celu odciążenia witryny z nadmiernej wysyłanych do niej zapytań i obecnie służy do kierowania botami internetowymi odwiedzającymi Twoją stronę internetową. 

###  O czym warto pamiętać przy tworzeniu pliku robots.txt?

Przy kreowaniu i konfigurowaniu robots.txt warto pamiętać o:
- Algorytmy serwerów HTTP są case-sensitive, czyli wrażliwe na wielkość liter. W związku z tym zaleca się, aby plik zawsze nazywał się robots.txt
- Najlepszym narzędziem do diagnostyki jest Google Search Console,
- W pliku robots.txt można również wstawiać pomocnicze komentarze. Powinny się one rozpoczynać od **# w nowej linii**.

</Content>


<Content title="Konfiguracja robots.txt">
W końcu przechodzimy do sedna artykułu, czyli do konfiguracji robots.txt. Ten plik jest w formacie zwykłego tekstu, zgodnego z Robots Exclusion Protocol. **Powinien on zawierać minimum jedną regułę.**

**Allow — zezwala** robotom indeksującym i botom wyszukiwarek na pobieranie informacji.

**Disallow — blokuje** dostęp do zawartości botom indeksującym i robotom wyszukiwarek. Przy stosowaniu dyrektywy disallow należy pamiętać o tym, że:
- disallow: /test - 
blokuje pliki oraz katalogi zaczynające się od  „test”, dlatego dobrą praktyką jest jeśli chcemy zablokować jedynie do folderu, należy dodać na końcu slash, a więc w rezultacie będzie **disallow: /test/,
- disallow: /*htm -
blokuje pliki htm oraz pliki html, co dla strony internetowej nigdy nie będzie mądrym rozwiązaniem dla procesu pozycjonowania stron, dlatego, aby zapobiec, warto dodać dyrektywę allow: /*html,

### Przykładowy plik robots.txt zastosowany w stronach internetowych postawionych na systemach zarządzania WordPress:

```
User-agent: *
Disallow: /wp-admin/
Allow: /wp-admin/admin-ajax.php

Sitemap: https://example.pll/wp-sitemap.xml
```
</Content>
<Content title="Roboty wyszukiwarek internetowych i plik robots.txt — Specjaliści RadWEB">
W filmach Marvela słyszałem, że z wielką mocą wiąże się wielka odpowiedzialność. Możliwość i wiedza na temat kontrolowania robotów internetowych jest wielką oraz pożądaną umiejętnością zarówno wśród programistów, twórców stron internetowych jak i specjalistów SEO. 
<br/>
**Dlatego też jeśli masz problem z poprawnym skonfigurowaniem meta tagów i pliku odpowiedzialnego za zarządzanie robotami, warto [skonsultować się ze specjalistami serwisu RadWEB](/kontakt).**
 
</Content>

<AuthorCard></AuthorCard>

export default ({children}) => <div>{children}</div>